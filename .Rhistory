table(Test$isB, PredictCART)
letter$letter = as.factor( letter$letter )
# Generating new data
set.seed(2000)
split = sample.split(letter$letter, SplitRatio = 0.5)
Train = subset(letter, split==TRUE)
Test = subset(letter, split==FALSE)
summary(Test)
0.2535302 + 0.2458280 + 0.2573813  + 0.2432606
census <- read.csv("C:/EMC/Cursos/Analytics Edge/census.csv")
View(census)
library(caTools)
set.seed(2000)
split = sample.split(census$over50k, SplitRatio = 0.6)
train = subset(census, split==TRUE)
test = subset(census, split==FALSE)
# Building logistic regression
censusLog = glm(over50k ~ ., data=train, family=binomial)
summary(censusLog)
library(UsingR)
data(father.son)
plot(father.son$fheight, father.son$sheight)
cor(father.son$fheight, father.son$sheight)
identify(father.son$fheight, father.son$sheight)
x = father.son$fheight
y = father.son$sheight
n = nrow(father.son)
plot(scale(x), scale(y))
abline(h=0, v=0)
sum(scale(x) * scale(y)) / (n - 1)
sum(scale(x) * scale(y))
sum(scale(x) * scale(y)) / n
data(nym.2002)
head(nym.2002)
histogram(nym.2002$time)
qqnorm(nym.2002$time)
tail(sort(table(nym.2002$home)),10)
time = sort(nym.2002$time)
summary(time)
max(time/median(time))
min(time/median(time))
babies = read.table("https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/babies.txt", header=TRUE)
bwt.nonsmoke = babies$bwt[babies$smoke==0]
bwt.smoke = babies$bwt[babies$smoke==1]
# QUESTION 1.1
# What is the average length of the confidence interval?
N=30
conf.int<-function(){
sample.bwt.nonsmoke <- sample(bwt.nonsmoke,N)
sample.bwt.smoke <- sample(bwt.smoke,N)
bwt.test <- t.test(sample.bwt.nonsmoke,sample.bwt.smoke)
return (bwt.test$conf.int[2] - bwt.test$conf.int[1])
}
mean(replicate(1000, conf.int()))
# QUESTION 1.2
# How often (what proportion of times) did the confidence intervals contain the population-level difference?
popdiff = mean(bwt.nonsmoke) - mean(bwt.smoke)
N=30
diff.compare<-function(){
sample.bwt.nonsmoke <- sample(bwt.nonsmoke,N)
sample.bwt.smoke <- sample(bwt.smoke,N)
bwt.test <- t.test(sample.bwt.nonsmoke,sample.bwt.smoke)
return (popdiff>bwt.test$conf.int[1] & popdiff<bwt.test$conf.int[2])
}
mean(replicate(1000, diff.compare()))
# QUESTION 1.3
# the difference in means (X.ns - X.s) must have absolute value greater than _____ times sd.diff in order for the result to be significant (at alpha=0.05).
# QUESTION 1.4
# the difference in means (X.ns - X.s) must be a greater distance than _____ times sd.diff away from 0 in order for the 95% confidence interval not to contain 0.
# QUESTION 3.1
# What is the power at alpha=0.1?
N=15
reject <- function(N,alpha){
sample.bwt.nonsmoke <- sample(bwt.nonsmoke,N)
sample.bwt.smoke <- sample(bwt.smoke,N)
pval <- t.test(sample.bwt.nonsmoke,sample.bwt.smoke)$p.value
ifelse(pval < alpha,1,0)
}
mean(replicate(1000,reject(N,0.1)))
# QUESTION 3.2
# What is the power at alpha=0.05?
mean(replicate(1000,reject(N,0.05)))
# QUESTION 3.3
# QUESTION 3.3
# What is the power at alpha=0.01?
mean(replicate(1000,reject(N,0.01)))
# QUESTION 3.4
# What is the expected number of student responses that would be marked wrong simply by chance?
# QUESTION 2.1
# What is the X-squared statistic?
d = read.csv("https://courses.edx.org/c4x/HarvardX/PH525.1x/asset/assoctest.csv")
d = read.csv("https://courses.edx.org/c4x/HarvardX/PH525.1x/asset/assoctest.csv")
d.table <- table(d)
chisq.test(d.table)$statistic
# QUESTION 2.2
# What is the p-value?
fisher.test(d.table)$p.value
babies = read.table("https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/babies.txt", header=TRUE)
bwt.nonsmoke = babies$bwt[babies$smoke==0]
bwt.smoke = babies$bwt[babies$smoke==1]
# QUESTION 1.1
# How often (what proportion of simulations) is the sample variance greater than 1.5 times the population variance?
pop.var = var(bwt.nonsmoke)
vars = replicate(1000, var(sample(bwt.nonsmoke,10)))
mean(vars > pop.var*1.5)
# QUESTION 1.2
# Now use a sample size of 50. How often (what proportion) is the sample variance larger than 1.5 times the population variance?
vars = replicate(1000, var(sample(bwt.nonsmoke,50)))
mean(vars > pop.var*1.5)
# QUESTION 2.1
# QUESTION 2.1
# What is the p-value for the two groups of 50 defined above?
set.seed(0)
N=50
smokers <- sample(babies$bwt[babies$smoke==1],N)
nonsmokers <- sample(babies$bwt[babies$smoke==0],N)
obs <- mean(smokers)-mean(nonsmokers)
avgdiff <- replicate(1000, {
all <- sample(c(smokers,nonsmokers))
smokersstar <- all[1:N]
nonsmokersstar <- all[(N+1):(2*N)]
return(mean(smokersstar) - mean(nonsmokersstar))
})
mean(abs(avgdiff) > abs(obs))
library("dplyr")
# QUESTION 1.1
# What is the median REM proportion of the order with the smallest median REM proportion?
msleep<-read.csv("https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/msleep_ggplot2.csv")
msleep %>%
mutate(rem_proportion = sleep_rem / sleep_total) %>%
group_by(order) %>%
summarise(median_rem_proportion = median(rem_proportion)) %>%
arrange(median_rem_proportion) %>%
head(1)
#======================================================================
# Week 4 Robust summaries
#======================================================================
data(ChickWeight)
chick = reshape(ChickWeight,idvar=c("Chick","Diet"),timevar="Time",direction="wide")
chick = na.omit(chick)
ChickWeight[ChickWeight$Chick==,]
data(ChickWeight)
chick = reshape(ChickWeight,idvar=c("Chick","Diet"),timevar="Time",direction="wide")
chick = na.omit(chick)
ChickWeight[ChickWeight$Chick==,]
data(ChickWeight)
plot(ChickWeight$Time, ChickWeight$weight, col=ChickWeight$Diet)
head(ChickWeight)
chick = reshape(ChickWeight,idvar=c("Chick","Diet"),timevar="Time",direction="wide")
head(chick)
chick = na.omit(chick)
mean(c(chick$weight.4,3000))/mean(chick$weight.4)
median(c(chick$weight.4,3000))/median(chick$weight.4)
sd(c(chick$weight.4,3000))/sd(chick$weight.4)
mad(c(chick$weight.4,3000))/mad(chick$weight.4)
mean(c(chick$weight.4, 3000))/mean(chick$weight.4)
median(c(chick$weight.4, 3000))/median(chick$weight.4)
sd(c(chick$weight.4, 3000))/sd(chick$weight.4)
cor(c(chick$weight.4,3000), c(chick$weight.21,3000)) / cor(chick$weight.4, chick$weight.21)
x<-subset(chick$weight.4,chick$Diet==1)
y<-subset(chick$weight.4,chick$Diet==4)
t.test(c(x,200),y)$p.value
data(ChickWeight)
plot(ChickWeight$Time, ChickWeight$weight, col=ChickWeight$Diet)
head(ChickWeight)
chick = reshape(ChickWeight,idvar=c("Chick","Diet"),timevar="Time",direction="wide")
head(chick)
chick = na.omit(chick)
mean(c(chick$weight.4,3000))/mean(chick$weight.4)
median(c(chick$weight.4,3000))/median(chick$weight.4)
sd(c(chick$weight.4,3000))/sd(chick$weight.4)
mad(c(chick$weight.4,3000))/mad(chick$weight.4)
plot(chick$weight.4~chick$weight.21)
cor(c(chick$weight.4,3000),c(chick$weight.21,3000))/cor(chick$weight.4,chick$weight.21)
cor(c(chick$weight.4,3000),c(chick$weight.21,3000), method = 'spearman')/cor(chick$weight.4,chick$weight.21,method = 'spearman')
t.test(x,y+10)$statistic-t.test(x,y+100)$statistic
x<-subset(chick$weight.4,chick$Diet==1)
y<-subset(chick$weight.4,chick$Diet==4)
t.test(c(x,200),y)$p.value
# QUESTION 2.2
# What is the difference in t-test statistic (the statistic is obtained by t.test(x,y)$statistic) between adding 10 and adding 100 to all the values in the group 'y'?
t.test(x,y+10)$statistic-t.test(x,y+100)$statistic
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(swirl)
install_from_swirl("Exploratory Data Analysis")
switrl()
swirl()
`repdata.data.StormData.(1)` <- read.csv("C:/EMC/Cursos/GitHub/datasciencecoursera/Reproducible_Research_Proy2/repdata-data-StormData (1).csv")
View(`repdata.data.StormData.(1)`)
install.packages(c("xts", "hwriter", "stringr", "optparse"))
install_from_swirl("Statistical Inference")
library(swirl)
install_from_swirl("Statistical Inference")
swirl()
?quni
?quni()
?qunif
?qunif(0.75, 0, 1)
qunif(0.75, 0, 1)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
tem
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
tem
temp
.1+.4+.9+1.6
(.1+.4+.9+1.6)/4
(1*.1)+(2*.2)+(3*.3)+(4*.4)
((1*.1)+(2*.2)+(3*.3)+(4*.4))/4
search()
library()
mean(1:1337)
?sd()
?median()
d <- c(1:10, 30:40, 5, 7, 9, 12)
median(d)
sum(10)
sum(d)
x <- sample(letters, 10)  ; x
x[x>10]
rep(LETTERS, 5)
dir(5)
dir()5
dir()[5]
length(builtins())
sample(LETTERS, 5, replace=F)
c(5,letters)
a <- sample(letters[1:4], 100, replace=T)
pie(table(a))
length <- rnorm(10, 180, 10)
length
weight <- (length/100)^2 * 25
measurements <- data.frame(cbind(length, weight))
measurements
measurements <- round(measurements)
measurements
str(measurements)
length <- rnorm(10, 180, 10)
length
weight <- (length/100)^2 * 25
measurements <- data.frame(cbind(length, weight))
measurements
str(measurements)
getwd()
unmet <- read.table("data_unmetneed1_1.csv",header=T,sep=",")
View(unmet)
View(unmet)
table(unmet$wtr)
pie(sort(table(unmet$wtr)))
barplot(table(unmet$edu), las=2)
hist(unmet$wtr)
barplot(unmet$wtr)
Q6 <- rnorm(1000)^2
Q6
plot(Q6)
hist(Q6)
summary(Q6)
x <- 0:30 ; plot(x, dbinom(x, 30, 0.5), type = "h")
Try the code x <- 0:2 ; plot(x, dbinom(x, 2, 0.5), type = "h")
x <- 0:2 ; plot(x, dbinom(x, 2, 0.5), type = "h")
x <- 0:2 ; plot(x, dbinom(x, 2, 0.5), type = "h")
x <- 0:2 ; plot(x, dbinom(x, 2, 0.5), type = "h")
x <- 0:2 ; plot(x, dbinom(x, 2, 0.5), type = "h")
x <- 0:2 ; plot(x, dbinom(x, 2, 0.5), type = "h")
x <- 0:2 ;
plot(x, dbinom( ), type = "h", col = "red", lwd=10, main="Probability that X patients are cured")
x <- 0:2 ;
plot(x, dbinom(0.7 ), type = "h", col = "red", lwd=10, main="Probability that X patients are cured")
x <- 0:2 ;
plot(x, dbinom(x, 2, 0.7 ), type = "h", col = "red", lwd=10, main="Probability that X patients are cured")
x <- 0:2
plot(x, dbinom(x, 2, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:2
plot(x, dbinom(x, 8, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:8
plot(x, dbinom(x, 2, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:8
plot(x, dbinom(x, 2, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:2
plot(x, dbinom(x, 8, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:7
plot(x, dbinom(x, 2, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:8
plot(x, dbinom(x, 2, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:8
plot(x, dbinom(x, 8, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 1, 0.8), add=T)
mean(x)
x <- 0:8
plot(x, dbinom(x, 8, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 4, 0.8), add=T)
mean(x)
x <- 0:8
plot(x, dbinom(x, 8, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 4, 1.5), add=T)
mean(x)
qnorm(.25,4,1.5)
qnorm(2,4,1.5)
qnorm(-2/1.5,4,1.5)
qnorm(2,4,1.5)
pnorm(.25,4,1.5)
pbinom(.25, size=8, prob=0.5, lower.tail=FALSE)
pbinom(.5, size=8, prob=0.5, lower.tail=FALSE)
pbinom(.5, size=8, prob=0.5)
pbinom(.5, size=8, prob=0.5)*100
pbinom(50, size=8, prob=0.5)*100
pbinom(2, size=8, prob=0.5)*100
pbinom(4, size=8, prob=0.5)*100
pbinom(1, size=8, prob=0.5)*100
pbinom(2, size=8, prob=0.5)*100
x <- 0:7
plot(x, dbinom(x, 8, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 4, 1.5), add=T)
mean(x)
pnorm(.25,4,1.5)
pbinom(2, size=8, prob=0.5)*100
x <- 0:8
plot(x, dbinom(x, 8, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 4, 1.5), add=T)
mean(x)
pnorm(.25,4,1.5)
pbinom(2, size=8, prob=0.5)*100
round(dbinom(2, 8, 0.5),2)
x <- 0:30
plot(x, dbinom(x, 30, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 15, 1.5), add=T)
sd(X)
sd(x)
curve(dnorm(x, 15, 9), add=T)
x <- 0:30
plot(x, dbinom(x, 30, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
mean(x)
sd(x)
curve(dnorm(x, 15, 9), add=T)
curve(dnorm(x, 15, 2.8), add=T)
sd(x)
sd(x)/sqrt(30)
curve(dnorm(x, 15, 5.6), add=T)
curve(dnorm(x, 15, 2.8), add=T)
x <- 0:30
plot(x, dbinom(x, 30, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 15, 2.8), add=T)
sd(x)/sqrt(30)
x
sd(dbinom(x, 30, 0.5)
sd(dbinom(x, 30, 0.5))
mean(x)
sd(dbinom(x, 30, 0.5))
dbinom(x, 30, 0.5)
mean(dbinom(x, 30, 0.5))
sqrt(30*0.5*(1-0.5))
sqrt(8*0.5*(1-0.5))
30*0.5*(1-0.5))
30*0.5*(1-0.5)
qnorm(0.25,15)
qnorm(0.25,0,1)
mean(dbinom(x, 30, 0.5))
sd(dbinom(x, 30, 0.5))
table(x, dbinom(x, 30, 0.5))
dbinom(x, 30, 0.5)
dbinom(x, 30, 0.5)*100
dbinom(x, 30, 0.5)
x <- 0:30
plot(x, dbinom(x, 30, 0.5), type = "h", col = "blue", lwd=4, ylim= c(0,0.6))
curve(dnorm(x, 15, 2.8), add=T)
sd(dbinom(x, 30, 0.5))
dbinom(x, 30, 0.5)
dbinom(x, 30, 0.5)*1000000
dbinom(x, 30, 0.5)
set.seed(400)
NORMAL <- rnorm(10000)
UNIFORM <- runif(10000)
SKEWED <- rep(1:140, 1:140)
opar <- par() #Save original par settings. Read ?par() if you like
par(mfrow= c(3,1)) #Ask for three columns and one row in the graph
hist(NORMAL)
hist(UNIFORM)
hist(SKEWED)
par(mfrow= c(3,1)) #Ask for three columns and one row in the graph
sampl <- vector() #Create an empty vector
for(i in 1:1000) #Start a loop with 1000 rounds
sampl <- c(sampl, mean(sample(NORMAL, 3, replace=T)))
#fill sampl with sampl, and the mean of three random items from NORMAL
mean(sampl)
sd(sampl)
hist(sampl, xlim = c(-2, 2), main = " n = 3 " )
sampl <- vector()
for(i in 1:1000)
sampl <- c(sampl, mean(sample(NORMAL, 6, replace=T)))
mean(sampl)
sd(sampl)
hist(sampl, xlim = c(-2, 2), main = " n = 6 " )
sampl <- vector()
for(i in 1:1000)
sampl <- c(sampl, mean(sample(NORMAL, 300, replace=T)))
mean(sampl)
sd(sampl)
hist(sampl, xlim = c(-2, 2), main = " n = 300 " )
?sample
par(mfrow= c(3,1)) #Ask for three columns and one row in the graph
sampl <- vector() #Create an empty vector
for(i in 1:1000) #Start a loop with 1000 rounds
sampl <- c(sampl, mean(sample(UNIFORM, 3, replace=T)))
#fill sampl with sampl, and the mean of three random items from NORMAL
mean(sampl)
sd(sampl)
hist(sampl, xlim = c(-2, 2), main = " n = 3 " )
sampl <- vector()
for(i in 1:1000)
sampl <- c(sampl, mean(sample(UNIFORM, 6, replace=T)))
mean(sampl)
sd(sampl)
hist(sampl, xlim = c(-2, 2), main = " n = 6 " )
sampl <- vector()
for(i in 1:1000)
sampl <- c(sampl, mean(sample(UNIFORM, 300, replace=T)))
mean(sampl)
sd(sampl)
hist(sampl, xlim = c(-2, 2), main = " n = 300 " )
x <- rnorm(100000)
y <- rnorm(100000)
z <- rep(NA, 100000) #z is created empty but with a given size.
system.time({
for (i in 1:100000) {
z[i] <- x[i] + y[i]
}
})
system.time( k <- x + y )
?predict()
set.seed(897)
ME <- matrix(rnorm(24000),nrow=1000)
View(ME)
View(ME)
colnames(ME) <- c(paste("A",1:12,sep=""),paste("B",1:12,sep=""))
View(ME)
View(ME)
length(which(ME>=0))
keep <- (apply(ME[,1:12],1,mean) > 0) & (apply(ME[,13:24],1,mean) > 0)
length(keep)
sum(keep) #make sure you understand sum() applied to logicals
head(keep)
trimmed <- apply(ME,1,function(ME){mean(ME, trim=0.05)})
trimmed
install_from_swirl("Statistical Inference")
library(swirl)
install_from_swirl("Statistical Inference")
swirl
swirl()
10
info()
q
1
2
3
4
5
6
7
8
11/12
deck
1/54
1/52
52
4/52
0
12/52
12/52
13/52
3/52
2/52
1/52
2/51
1.6*0.8/2
.64
mypdf
mypdf(1.6)
integrate(mypdf, 0, 1.6)
.32
sqrt(2)
setwd("c:/EMC/Cursos/GitHub/Practice-With-Titanic/")
train <- read.csv("train.csv")
test <- read.csv("test.csv")
library(caTools)
library(ROCR)
library(caret)
library(e1071)
library(randomForest)
library(flexclust)
summary(train)
test[which(is.na(test$Fare)), ]
